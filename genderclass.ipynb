{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enableGenderIcons = True\n",
    "\n",
    "male_icon = cv2.imread(\"male.jpg\")\n",
    "male_icon = cv2.resize(male_icon, (40, 40))\n",
    "\n",
    "female_icon = cv2.imread(\"female.jpg\")\n",
    "female_icon = cv2.resize(female_icon, (40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('/Users/Aveen Faheem/Desktop/Anaconda_work/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadVggFaceModel():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ageModel():\n",
    "    model = loadVggFaceModel()\n",
    "    \n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(101, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation('softmax')(base_model_output)\n",
    "    \n",
    "    age_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "    \n",
    "    #you can find the pre-trained weights for age prediction here: https://drive.google.com/file/d/1YCox_4kJ-BYeXq27uUbasu--yz28zUMV/view?usp=sharing\n",
    "    age_model.load_weights(\"/Users/Aveen Faheem/Desktop/Anaconda_work/age_model_weights.h5\")\n",
    "    \n",
    "    return age_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderModel():\n",
    "    model = loadVggFaceModel()\n",
    "    \n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(2, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "    gender_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "    \n",
    "    #you can find the pre-trained weights for gender prediction here: https://drive.google.com/file/d/1wUXRVlbsni2FN9-jkS_f4UTUrm1bRLyk/view?usp=sharing\n",
    "    gender_model.load_weights(\"/Users/Aveen Faheem/Desktop/Anaconda_work/gender_model_weights.h5\")\n",
    "    \n",
    "    return gender_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aveen Faheem\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Aveen Faheem\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "age_model = ageModel()\n",
    "gender_model = genderModel()\n",
    "\n",
    "#age model has 101 outputs and its outputs will be multiplied by its index label. sum will be apparent age\n",
    "output_indexes = np.array([i for i in range(0, 101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception could not broadcast input array from shape (40,40,3) into shape (0,40,3)\n",
      "exception OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3718: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) #capture webcam\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    #img = cv2.resize(img, (640, 360))\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 130: #ignore small faces\n",
    "\n",
    "#mention detected face\n",
    "            \"\"\"overlay = img.copy(); output = img.copy(); opacity = 0.6\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),cv2.FILLED) #draw rectangle to main image\n",
    "            cv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\"\"\"\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),1) #draw rectangle to main image\n",
    "\n",
    "            #extract detected face\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "\n",
    "            try:\n",
    "                #age gender data set has 40% margin around the face. expand detected face.\n",
    "                margin = 30\n",
    "                margin_x = int((w * margin)/100); margin_y = int((h * margin)/100)\n",
    "                detected_face = img[int(y-margin_y):int(y+h+margin_y), int(x-margin_x):int(x+w+margin_x)]\n",
    "            except:\n",
    "                print(\"detected face has no margin\")\n",
    "\n",
    "            try:\n",
    "                #vgg-face expects inputs (224, 224, 3)\n",
    "                detected_face = cv2.resize(detected_face, (224, 224))\n",
    "                \n",
    "                img_pixels = image.img_to_array(detected_face)\n",
    "                img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "                img_pixels /= 255\n",
    "                \n",
    "                #find out age and gender\n",
    "                age_distributions = age_model.predict(img_pixels)\n",
    "                apparent_age = str(int(np.floor(np.sum(age_distributions * output_indexes, axis = 1))[0]))\n",
    "\n",
    "                gender_distribution = gender_model.predict(img_pixels)[0]\n",
    "                gender_index = np.argmax(gender_distribution)\n",
    "\n",
    "                if gender_index == 0: gender = \"F\"\n",
    "                else: gender = \"M\"\n",
    "\n",
    "                #background for age gender declaration\n",
    "                info_box_color = (46,200,255)\n",
    "                #triangle_cnt = np.array( [(x+int(w/2), y+10), (x+int(w/2)-25, y-20), (x+int(w/2)+25, y-20)] )\n",
    "                triangle_cnt = np.array( [(x+int(w/2), y), (x+int(w/2)-20, y-20), (x+int(w/2)+20, y-20)] )\n",
    "                cv2.drawContours(img, [triangle_cnt], 0, info_box_color, -1)\n",
    "                cv2.rectangle(img,(x+int(w/2)-50,y-20),(x+int(w/2)+50,y-90),info_box_color,cv2.FILLED)\n",
    "\n",
    "                #labels for age and gender\n",
    "                cv2.putText(img, apparent_age, (x+int(w/2), y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
    "\n",
    "                if enableGenderIcons:\n",
    "                    if gender == 'M': gender_icon = male_icon\n",
    "                    else: gender_icon = female_icon\n",
    "\n",
    "                    img[y-75:y-75+male_icon.shape[0], x+int(w/2)-45:x+int(w/2)-45+male_icon.shape[1]] = gender_icon\n",
    "                else:\n",
    "                    cv2.putText(img, gender, (x+int(w/2)-42, y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"exception\",str(e))\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
